package com.github.tjake.jlama.model;

import com.github.tjake.jlama.safetensors.DType;
import com.github.tjake.jlama.safetensors.SafeTensorSupport;
import org.junit.Test;

import java.io.File;
import java.io.IOException;
import java.util.UUID;


public class TestSample {

    @Test
    public void sample() throws IOException {
        String model = "tjake/TinyLlama-1.1B-Chat-v1.0-Jlama-Q4";
        String workingDirectory = "./models";

        String prompt = "What is the best season to plant avocados?";

        // Downloads the model or just returns the local path if it's already downloaded
        File localModelPath = SafeTensorSupport.maybeDownloadModel(workingDirectory, model);

        // Loads the model
        AbstractModel m = ModelSupport.loadModel(localModelPath, DType.F32, DType.I8);

        // Checks if the model supports chat prompting and adds prompt in the expected format for this model
        if (m.promptSupport().isPresent()) {
            prompt = m.promptSupport().get().newBuilder()
                    .addSystemMessage("You are a helpful chatbot who writes short responses.")
                    .addUserMessage(prompt)
                    .build();
        }

        System.out.println("Prompt: " + prompt + "\n");
        //Streams each token generated by the model to the console
        m.generate(UUID.randomUUID(), prompt, 0.7f, 256, false, (s, f) -> System.out.print(s));
        System.out.println();
    }
}
